# -*- coding: utf-8 -*-
"""2142745_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Y1OrxbsuxmyRl-JnDEA_Cb_a0mPzyZL

# **MSc Project Code - 2142745**

The purpose of this notebook is to create a synthetic dataset to use for machine learning experimentation. We currently do not have access to the real Hywel Dda data that will be used going forwards in the PhD, therefore I created artificial data that mimics real data. This notebook also contains multiple ML models, with results for each. There are also tests with noise in the dataset, and performance has been measured across 3 different dataset sizes.

Importing the necessary packages. Some of these may have been used in previous iterations of the code but may not be used in the submitted final version.
"""

import numpy as np
import pandas as pd
import sklearn
from scipy.stats import binom
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.model_selection import cross_val_score
from sklearn import svm
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
from sklearn.model_selection import train_test_split
import tensorflow as tf
from google.colab import files
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn import datasets, svm
from sklearn.model_selection import train_test_split
import keras
from tensorflow.keras.layers import Conv1D
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import BatchNormalization
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import f1_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
from tensorflow.keras.layers import MaxPooling1D
from tensorflow.keras.layers import GlobalAveragePooling1D

"""# **Data Generation**

This section will create the synthetic data points by using NumPy's random modules.
"""

size = 25000        #setting size variable which can be easily changed.

age1 = np.random.normal(41, 15 ,size)       #choosing random numbers with a normal distribution
age = []
for elem in age1:
  ele = round(elem)         #making sure numbers are whole.
  age.append(ele)

print(age)

gender_r = np.random.random(size)     #randomly selecting numbers between 0 and 1
print(gender_r)
gender = []
for elem in gender_r:
  ele = round(elem)       #rounding to the closest 0 and 1 - 0 represents male, 1 represents female.
  gender.append(ele)

print(gender)

j = 0                                       #this cell just checks the numbers for each to make sure it's reasonably equal.
k = 0
gender_array = np.asarray(gender)
for elem in gender_array:
  if elem == 0:
    j = j + 1
  if elem == 1:
    k = k + 1
print(j,k)

physical_health = np.random.choice([0,1,2,3,4],size=size,p=[0.5,0.2,0.1,0.1,0.1])     #generating random numbers for multiple health conditions
print(physical_health)

simple_health = np.random.choice([0,1],size=size,p=[0.7,0.3])     #generating 0 or 1 for health conditions - present or not. Used in previous versions

mental_health = health = np.random.choice([0,1],size=size,p=[0.8,0.2])    #generating mental health

smoking = np.random.choice([0,1],size=size,p=[0.85,0.15])       #generating smoking

regular_drinker = np.random.choice([0,1],size=size,p=[0.4,0.6])     #generating regular drinking

regular_exercise = health = np.random.choice([0,1,2],size=size,p=[0.3,0.5,0.2])   #generating exercise levels

ses = np.random.choice([0,1,2], size=size, p=[0.2,0.5,0.3])         #generating socio-economic status

"""Health labels:  0 > none
                1 > respiratory
                2 > cardiovascular
                3 > neurological
                4 > cancer.

                Different odds for each one.

SES: 0 - high SES/low impact, 1 - average, 2- low SES/high impact

Exercise: 0 > Exercises properly/low impact, 2 > low to moderate exercise, 3 > no exercise/high impact

Typical trend is 0 has less impact, 1 or 2 have increasing impact.

# **Creating a Dataframe**

This section will use the generated data to create a DataFrame for organising and handling the data.
"""

data = {'Age': age, 'Gender': gender, 'Physical Health': physical_health, 'Mental Health': mental_health, 'Smoking': smoking, 'Alcohol Consumption': regular_drinker, 'Exercise': regular_exercise, 'Socio-economic Status': ses}

health_df = pd.DataFrame(data=data)   #using the above dict to create DF

print(health_df)

"""**Saving dataset with no labels to pickle**"""

health_df.to_pickle('/content/drive/MyDrive/PhD - MSc Part/Data/final_data.pkl')    #save dataset

df = pd.read_pickle('/content/drive/MyDrive/PhD - MSc Part/Data/final_data.pkl')    #load dataset

"""# **Calculating odds of increased LoS**

This section will use the previously saved/loaded DataFrame, and will generate labels. This is done using the below functions for generating a patient's odds of staying for longer in the hospital. Then, a random number will be generated, and the higher the odds, the higher the probability that this number will fall within the threshold for the higher category.
"""

def age_odds(row):
  age_odds = row['Age']*0.30          #odds increase at a rate of 0.3% for each year
  return age_odds

def get_odds(index,row):                  #standard odds generation function (no noise)
  odds = 0
  if row['Gender']== 1:                   #if certain features are present, odds increase by specified amount
    odds +=5
  if row['Physical Health'] == 1:
    odds+= 25
  if row['Physical Health'] == 2:
    odds+= 15
  if row['Physical Health'] == 3:
    odds+= 20
  if row['Physical Health'] == 4:
    odds+= 20
  if row['Mental Health'] == 1:
    odds += 20
  if row['Exercise'] == 1:
    odds+=2
  if row['Exercise'] == 2:
    odds+=5
  if row['Smoking']== 1:
    odds+=10
  if row['Alcohol Consumption'] == 1:
    odds+= 12
  if row['Socio-economic Status'] == 1:
    odds+= 5
  if row['Socio-economic Status'] == 2:
    odds+=10
  age_odd = age_odds(row)
  return odds + age_odd

def get_odds_noise(index,row):              #odds function that adds noise by randomly generating odds increases with large ranges
  odds = 0                                  #I did try smaller ranges but it didn't add enough noise and models did fine
  if row['Gender']== 1:
    odds +=np.random.randint(8,15)
  if row['Physical Health'] == 1:
    odds+= np.random.randint(12,60)
  if row['Physical Health'] == 2:
    odds+= np.random.randint(12,60)
  if row['Physical Health'] == 3:
    odds+= np.random.randint(12,60)
  if row['Physical Health'] == 4:
    odds+= np.random.randint(15,65)
  if row['Mental Health'] == 1:
    odds += np.random.randint(20,60)
  if row['Exercise'] == 1:
    odds+=np.random.randint(10,20)
  if row['Exercise'] == 2:
    odds+=np.random.randint(10,25)
  if row['Smoking']== 1:
    odds+=np.random.randint(10,45)
  if row['Alcohol Consumption'] == 1:
    odds+= np.random.randint(5,40)
  if row['Socio-economic Status'] == 1:
    odds+= np.random.randint(2,20)
  if row['Socio-economic Status'] == 2:
    odds+= np.random.randint(2,25)
  age_odd = age_odds(row)
  return odds + age_odd

odds_list = []                          #creating list of each patient's odds

for index, row in df.iterrows():
  odds = get_odds(index,row)
  odds_list.append(odds)

print(odds_list)
odds_list = np.asarray(odds_list)
max= np.max(odds_list)

new_odds = []
for elem in odds_list:
  elem = elem/max
  new_odds.append(elem)

print(new_odds)

new_odds = np.asarray(new_odds)
max= np.max(new_odds)
print(max)

odds_noise_list = []                #same as above but for noisy data

for index, row in df.iterrows():
  odds = get_odds_noise(index,row)
  odds_noise_list.append(odds)

odds_noise_list =np.asarray(odds_noise_list)
noise_max = np.max(odds_noise_list)

new_noise_odds = []
for elem in odds_noise_list:
  elem = elem/noise_max
  new_noise_odds.append(elem)

print(np.max((new_noise_odds)))

def label_generation(odds_list):
    random = np.random.rand(1)
    if random <= odds_list:
      cat = 1
    else:
      cat = 0
    return cat

def label_noise_generation(new_noise_odds):
    random = np.random.rand(1)
    if random <= new_noise_odds:
      cat = 1
    else:
      cat = 0
    return cat

labels = []                 #creating empty labels list
for odd in new_odds:
  odds = label_generation(odd)     #fill labels list with labels generated using the function
  labels.append(odds)

labels = np.asarray(labels)

print(labels.shape)

labels = []
label = label_generation(new_odds)

labels_noise = []                     #as above, but for noisy data
for odd_noise in new_noise_odds:
  odds = label_noise_generation(odd_noise)
  labels_noise.append(odds)

print(labels_noise)     #check labels

"""**Checking numbers and saving LoS as labels, then saving as a new pickle**"""

df['LoS'] = labels     #setting labels in the df as the labels list
#df['LoS'] = labels           #2 versions, one for normal and one for noise, one will be commented out

df.to_pickle("/content/drive/MyDrive/PhD - MSc Part/Data/finaldata_withlabel.pkl")      #saving data with no noise

df.to_pickle("/content/drive/MyDrive/PhD - MSc Part/Data/finaldata_withnoise.pkl")      #saving data with noise

df = pd.read_pickle("/content/drive/MyDrive/PhD - MSc Part/Data/finaldata_withlabel.pkl")       #loading data without noise

df = pd.read_pickle("/content/drive/MyDrive/PhD - MSc Part/Data/finaldata_withnoise.pkl")     #loading data with noise

"""# **Visualizing data**"""

df = df.drop(df[df['Age'] < 0].index)

hist_age = df['Age'].hist(bins=25)

df['Socio-economic Status'].plot(kind='kde')

df['Gender'].plot(kind='kde')

df['Smoking'].plot(kind='kde')

df['Alcohol Consumption'].plot(kind='kde')

df['Physical Health'].plot(kind='kde')

df['Mental Health'].plot(kind='kde')

df['Socio-economic Status'].plot(kind='kde')

df['Exercise'].plot(kind='kde')

df['Exercise'].plot(kind='kde')

df['LoS'].plot(kind='kde')

print(df)

hist_exercise = df['Exercise'].hist(bins=3)



"""The following two cells are just used to randomly sample rows from the dataset, so that different sizes can be tested."""

fifteenk_sample = df.sample(n=15000)      #getting dataset of 15k samples

fivek_sample = df.sample(n=15000)       #getting dataset of 5k samples

"""# **Machine Learning**

This section will now use the full dataset to perform ML experiments. Results are also included.
"""

y = df['LoS'].values          #y values are the LoS labels
X = df[['Age', 'Gender', 'Physical Health', 'Mental Health', 'Smoking', 'Alcohol Consumption', 'Exercise', 'Socio-economic Status']].values

"""The following two cells are commented out in case of all cells being run, as these were used to get scores for different dataset sizes. The results for this can be found further below."""

#y = fifteenk_sample['LoS'].values
#X = fifteenk_sample[['Age', 'Gender', 'Physical Health', 'Mental Health', 'Smoking', 'Alcohol Consumption', 'Exercise', 'Socio-economic Status']].values

#y = fivek_sample['LoS'].values
#X = fivek_sample[['Age', 'Gender', 'Physical Health', 'Mental Health', 'Smoking', 'Alcohol Consumption', 'Exercise', 'Socio-economic Status']].values

"""**Random Forest**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)      #get train test split

clf = RandomForestClassifier(max_depth=16, random_state=6, n_estimators=128, max_features=8)      #initializing

clf.fit(X_train, y_train)               #fit to data
pred_clf = clf.predict(X_test)       #predict (for scores)

score = clf.score(X_test,y_test)        #get score
f1 = f1_score(y_test, pred_clf, average='weighted')  #get f1

print(score)
print("F1 score: ", f1)

#F1_tree_25k = f1
#F1_tree_15k = f1
#F1_tree_5k = f1
#F1_tree_noise = f1
#tree_f1 = f1

cm = confusion_matrix(y, pred_clf)

cm_display = ConfusionMatrixDisplay(cm).plot()

"""**Splitting into training and testing**

**Scaling**
"""

scaler = StandardScaler()                 #scale data
scaler.fit(X_train)
scaled_train = scaler.transform(X_train)
scaled_test = scaler.transform(X_test)

"""**SVC**"""

svm = SVC(gamma='auto')    #same process as random forest
svm.fit(scaled_train, y_train)
pred = svm.predict(scaled_test)

score_svm = svm.score(scaled_test, y_test)
f1 = f1_score(y_test, pred)
print("Score: ", score_svm)
print("F1: ", f1)

#F1_svm_25k = f1
#F1_svm_15k = f1
#F1_svm_5k = f1
#f1_svm_noise = f1
#svm_f1 = f1

cm = confusion_matrix(y_test, pred)

cm_display = ConfusionMatrixDisplay(cm).plot()

"""**KNN**"""

KNN = KNeighborsClassifier(n_neighbors=5)   #same prcoess as above
KNN.fit(X_train, y_train)
pred_KNN = KNN.predict(X_test)

score = KNN.score(X_test, y_test)
f1 = f1_score(pred_KNN, y_test, average='weighted')

print("Score: ", score)
print("F1: ", f1)

k_values = [i for i in range (1,31)]
scores = []

scaler = StandardScaler()
X = scaler.fit_transform(X)

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    score = cross_val_score(knn, X, y, cv=5)
    scores.append(np.mean(score))

import seaborn as sns
sns.lineplot(x = k_values, y = scores, marker = 'o')
plt.xlabel("K Values")
plt.ylabel("Accuracy Score")

#F1_KNN_25k = f1
#F1_KNN_15k = f1
#F1_KNN_5k = f1
#F1_KNN_noise = f1
#KNN_f1 = f1

cm = confusion_matrix(y, pred_KNN)

cm_display = ConfusionMatrixDisplay(cm).plot()

"""**K-means**

The K-means classifier consistently performs badly, therefore this has been excluded from further tests and just serves as evidence of poor performance.
"""

kmeans = KMeans(n_clusters=2, random_state=0, n_init=3).fit(X)
pred_kmeans = kmeans.predict(X)

score = kmeans.score(X)
f1 = f1_score(pred_kmeans, y, average='weighted')

print("Score: ", score)
print("F1: ", f1)

kmeans_f1 = f1

cm = confusion_matrix(y, pred_kmeans)

cm_display = ConfusionMatrixDisplay(cm).plot()

"""**Neural Network**"""

model = tf.keras.Sequential(layers=[
    tf.keras.layers.Dense(128, activation=tf.nn.relu), #dense layer
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dropout(.1),#dropout to prevent overfitting
    tf.keras.layers.Dense(128, activation=tf.nn.relu),
    tf.keras.layers.Dense((1), activation=tf.nn.sigmoid)])      #number of neurons = number of potential classes

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1e-6),     #low learning rate to prevent overfitting
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=tf.keras.metrics.BinaryAccuracy())       #sparse categorical is good for multiple classes

history = model.fit(scaled_train, y_train, epochs=50, validation_split=0.2, verbose=1)     #validation split which could be better with presplit val data but is fine for this task

score_NN = model.evaluate(scaled_test, y_test)
print(f'Test set loss: {score_NN[0]:0.2f}, test set accuracy: {score_NN[1]*100:0.2f}%')     #change into percentage

model2 = tf.keras.Sequential(layers=[
    tf.keras.layers.Dense(9, activation=tf.nn.relu),
    tf.keras.layers.Dense(8, activation=tf.nn.relu),
    tf.keras.layers.Dense((1), activation=tf.nn.sigmoid)])

model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),     #low learning rate to prevent overfitting
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=tf.keras.metrics.BinaryAccuracy())

history = model2.fit(scaled_train, y_train, epochs=50, validation_split=0.2, verbose=1)

score_NN = model2.evaluate(scaled_test, y_test)
print(f'Test set loss: {score_NN[0]:0.2f}, test set accuracy: {score_NN[1]*100:0.2f}%')

#F1_NN_25k = score_NN
#F1_NN_15k = score_NN
F1_NN_5k = score_NN
#F1_NN_noise = score_NN
#NN_F1 = score_NN[1]

plt.figure(figsize=[10,5])     #plotting loss and accuracy curves
plt.subplot(121)
plt.plot(history.history['binary_accuracy'])
plt.plot(history.history['val_binary_accuracy'])
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Training Accuracy',
           'Validation Accuracy'])
plt.title('Accuracy Curves')

plt.subplot(122)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Training Loss',
           'Validation Loss'])
plt.title('Loss Curves')
plt.show()

"""**Graphs for trials**

The following cells have generated two graphs. The first shows the performance of the different models on the dataset when it's of different sizes, with performance staying consistent. The second graph depicts accuracy when the dataset has noise added in to make it difficult.
"""

data_NN = {'5000 Samples': F1_NN_5k[1]*100, '15000 Samples': F1_NN_15k[1]*100, '25000 Samples': NN_F1*100} #saving values and names as keys
names_NN = list(data_NN.keys())
values_NN = list(data_NN.values())
data_RF = {'5000 Samples': F1_tree_5k*100, '15000 Samples': F1_tree_15k*100, '25000 Samples': tree_f1*100}    #each one multiplied by 100 to get percentage
names_RF = list(data_RF.keys())
values_RF = list(data_RF.values())
data_KNN = {'5000 Samples': F1_KNN_5k*100, '15000 Samples': F1_KNN_15k*100, '25000 Samples': KNN_f1*100}
names_KNN = list(data_KNN.keys())
values_KNN = list(data_KNN.values())
data_SVM = {'5000 Samples': F1_svm_5k*100, '15000 Samples': F1_svm_15k*100, '25000 Samples': svm_f1*100}
names_SVM = list(data_SVM.keys())
values_SVM = list(data_SVM.values())
plt.plot(names_NN, values_NN, label='Neural Network')       #line plots on one graph
plt.plot(names_RF, values_RF, label='Random Forest')
plt.plot(names_KNN, values_KNN, label='K-Nearest Neighbour')
plt.plot(names_SVM, values_SVM, label='Support Vector Machine')
plt.ylim(0,100)     #accuaracy is from 0 to 100
plt.legend()
plt.ylabel('F1 Score')
plt.xlabel('Sample Size')
plt.title('F1 Score Across Different Dataset Sizes')
plt.show()

plt.bar("Random Forest",F1_tree_noise*100)        #bar graph to show accuracies with noisy dataset
plt.bar("SVM",f1_svm_noise*100)
plt.bar("KNN",F1_KNN_noise*100)
plt.bar("Neural Network",F1_NN_noise[1]*100)
plt.title("Accuracies of Classifiers with Noisy Data")
plt.ylim(0,100)
plt.ylabel("F1")
plt.xlabel("Classifier")
plt.show()

"""Summary of all F1 scores for normal data."""

print("F1 of Random Forest: ", tree_f1*100)
print("F1 of SVM: ", svm_f1*100)
print("F1 of KNN: ", KNN_f1*100)
print("F1 of Neural Network: ", NN_F1*100)

print("New F1 of K-Means: ", kmeans_f1*100)

"""Summary of all F1 scores for noisy data."""

print("F1 of Random Forest with noisy data: ", F1_tree_noise*100)
print("F1 of SVM with noisy data: ", f1_svm_noise*100)
print("F1 of KNN with noisy data: ", F1_KNN_noise*100)
print("F1 of Neural Network with noisy data: ", F1_NN_noise[1]*100)

"""Finish point. All processes and results explained in greater detail in accomponying dissertation."""